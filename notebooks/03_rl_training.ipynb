{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Entrenamiento por Aprendizaje por Refuerzo (RL)\n",
    "\n",
    "Este notebook describe el proceso de entrenamiento de los modelos de RL que forman parte\n",
    "del sistema de trading. Se cubren:\n",
    "\n",
    "1. Fundamentos de RL aplicado a trading\n",
    "2. Algoritmo PPO (Proximal Policy Optimization)\n",
    "3. Optimización de hiperparámetros con Optuna (Phase 3)\n",
    "4. Entrenamiento 30-fold ensemble (Phase 4)\n",
    "5. Prevención de sobreajuste: control de rollouts\n",
    "\n",
    "> **Nota**: Se presenta la metodología general. Los detalles de la función de recompensa\n",
    "> y la arquitectura exacta del agente son confidenciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RL aplicado a trading\n",
    "\n",
    "### ¿Por qué Aprendizaje por Refuerzo?\n",
    "\n",
    "A diferencia de los enfoques supervisados (clasificación/regresión), el RL permite:\n",
    "- **Decisiones secuenciales**: Cada acción afecta el estado futuro\n",
    "- **Optimización del resultado final**: No predice precio, sino maximiza P&L\n",
    "- **Gestión de posición**: Decide cuándo mantener, no solo cuándo entrar\n",
    "\n",
    "### Formulación como MDP\n",
    "\n",
    "| Componente | Descripción |\n",
    "|------------|-------------|\n",
    "| **Estado (s)** | Indicadores técnicos + información de posición actual |\n",
    "| **Acción (a)** | HOLD / CLOSE (decisión binaria sobre posición abierta) |\n",
    "| **Recompensa (r)** | Función que incentiva beneficio y penaliza riesgo |\n",
    "| **Transición** | Avance al siguiente periodo temporal (determinista) |\n",
    "\n",
    "### PPO vs SAC\n",
    "\n",
    "Se evaluaron dos algoritmos:\n",
    "\n",
    "| Algoritmo | Tipo | Resultado |\n",
    "|-----------|------|----------|\n",
    "| **PPO** (Proximal Policy Optimization) | On-policy | Resultados consistentes y reproducibles |\n",
    "| **SAC** (Soft Actor-Critic) | Off-policy | Resultados no reproducibles, descartado |\n",
    "\n",
    "PPO fue seleccionado por su **estabilidad** y **reproducibilidad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: convergencia típica de PPO en trading\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simular curva de aprendizaje realista\n",
    "timesteps = np.arange(0, 25000, 100)\n",
    "base_reward = -5 + 15 * (1 - np.exp(-timesteps / 8000))\n",
    "noise = np.random.normal(0, 2, len(timesteps))\n",
    "episode_rewards = base_reward + noise\n",
    "\n",
    "# Media móvil\n",
    "window = 20\n",
    "smooth_rewards = pd.Series(episode_rewards).rolling(window).mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Curva de aprendizaje\n",
    "axes[0].scatter(timesteps, episode_rewards, alpha=0.15, s=5, color='steelblue')\n",
    "axes[0].plot(timesteps, smooth_rewards, color='red', linewidth=2, label=f'Media móvil ({window} ep.)')\n",
    "axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Timesteps', fontsize=11)\n",
    "axes[0].set_ylabel('Recompensa por Episodio', fontsize=11)\n",
    "axes[0].set_title('Curva de Aprendizaje PPO (ejemplo)', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribución de recompensas: inicio vs final\n",
    "early = episode_rewards[:50]\n",
    "late = episode_rewards[-50:]\n",
    "axes[1].hist(early, bins=20, alpha=0.6, color='lightcoral', label='Primeros 50 episodios')\n",
    "axes[1].hist(late, bins=20, alpha=0.6, color='green', label='Últimos 50 episodios')\n",
    "axes[1].set_xlabel('Recompensa', fontsize=11)\n",
    "axes[1].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[1].set_title('Distribución de Recompensas: Antes vs Después', fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prevención de sobreajuste: Control de rollouts\n",
    "\n",
    "### El problema\n",
    "\n",
    "Un **rollout** es un pase completo por los datos de entrenamiento.\n",
    "Demasiados rollouts causan que el modelo **memorice** los datos en vez de aprender patrones.\n",
    "\n",
    "```\n",
    "Rollouts = Timesteps / Tamaño_datos\n",
    "```\n",
    "\n",
    "### Niveles de riesgo\n",
    "\n",
    "| Rollouts | Riesgo | Impacto |\n",
    "|----------|--------|---------|\n",
    "| 2-3x | Bajo | Aprendizaje óptimo |\n",
    "| 5-10x | Medio | Sobreajuste moderado |\n",
    "| 10-40x | Alto | Sobreajuste severo |\n",
    "| 40x+ | Crítico | Memorización total |\n",
    "\n",
    "### Ejemplo real del proyecto\n",
    "\n",
    "Con 12,367 velas de datos y timesteps fijos de 500,000:\n",
    "- `500,000 / 12,367 = 40.4 rollouts` → **sobreajuste extremo**\n",
    "\n",
    "La solución: `timesteps = tamaño_datos × 2` (máximo 3x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración: impacto del número de rollouts en sobreajuste\n",
    "np.random.seed(42)\n",
    "\n",
    "rollout_levels = [2, 5, 10, 20, 40]\n",
    "train_pfs = [1.3, 1.5, 1.8, 2.5, 3.8]  # PF en training\n",
    "test_pfs = [1.25, 1.15, 0.95, 0.72, 0.55]  # PF en test (datos nuevos)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(rollout_levels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_pfs, width, label='PF Training (in-sample)',\n",
    "               color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, test_pfs, width, label='PF Test (out-of-sample)',\n",
    "               color='crimson', alpha=0.8)\n",
    "\n",
    "ax.axhline(1.0, color='red', linestyle='--', alpha=0.5, label='Breakeven (PF=1.0)')\n",
    "ax.set_xlabel('Número de Rollouts', fontsize=12)\n",
    "ax.set_ylabel('Profit Factor', fontsize=12)\n",
    "ax.set_title('Sobreajuste: Training PF vs Test PF según rollouts', fontsize=13)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{r}x' for r in rollout_levels])\n",
    "ax.legend()\n",
    "\n",
    "# Anotar la zona peligrosa\n",
    "ax.annotate('Zona óptima\\n(2-3x)', xy=(0, 1.3), fontsize=10, color='green',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "ax.annotate('¡Sobreajuste\\nsevero!', xy=(4, 3.8), fontsize=10, color='red',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Degradación por rollouts:')\n",
    "for rollouts, train_pf, test_pf in zip(rollout_levels, train_pfs, test_pfs):\n",
    "    degradation = (train_pf - test_pf) / train_pf * 100\n",
    "    status = 'OK' if degradation < 15 else 'SOBREAJUSTE'\n",
    "    print(f'  {rollouts:2d}x: Train PF={train_pf:.2f}, Test PF={test_pf:.2f}, '\n",
    "          f'Degradación={degradation:.0f}% [{status}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimización de hiperparámetros con Optuna (Phase 3)\n",
    "\n",
    "Optuna realiza **50 trials** de optimización bayesiana para encontrar\n",
    "los mejores hiperparámetros de PPO para cada señal.\n",
    "\n",
    "### Hiperparámetros optimizados (ejemplo)\n",
    "- Learning rate\n",
    "- Número de steps por update\n",
    "- Tamaño de batch\n",
    "- Número de épocas\n",
    "- Coeficiente de entropía\n",
    "- Factor de descuento (gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de optimización Optuna\n",
    "np.random.seed(42)\n",
    "\n",
    "n_trials = 50\n",
    "trials = np.arange(n_trials)\n",
    "\n",
    "# Simular PFs de Optuna: mejora gradual con ruido\n",
    "base_pf = 0.8 + 0.6 * (1 - np.exp(-trials / 15)) + np.random.normal(0, 0.15, n_trials)\n",
    "base_pf = np.clip(base_pf, 0.3, 2.5)\n",
    "best_so_far = np.maximum.accumulate(base_pf)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PF por trial\n",
    "axes[0].scatter(trials, base_pf, alpha=0.6, s=40, color='steelblue', label='PF por trial')\n",
    "axes[0].plot(trials, best_so_far, color='red', linewidth=2, label='Mejor hasta ahora')\n",
    "axes[0].axhline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Trial', fontsize=11)\n",
    "axes[0].set_ylabel('Profit Factor', fontsize=11)\n",
    "axes[0].set_title('Optimización Optuna: PF por Trial', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribución de PFs\n",
    "axes[1].hist(base_pf, bins=25, color='steelblue', alpha=0.7, edgecolor='white')\n",
    "axes[1].axvline(base_pf.max(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mejor: PF={base_pf.max():.2f}')\n",
    "axes[1].axvline(1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Profit Factor', fontsize=11)\n",
    "axes[1].set_title('Distribución de PF en 50 Trials', fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Mejor PF encontrado: {base_pf.max():.2f} (trial {base_pf.argmax()})')\n",
    "print(f'PF medio: {base_pf.mean():.2f}')\n",
    "print(f'Trials con PF > 1.0: {(base_pf > 1.0).sum()} / {n_trials}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento 30-Fold Ensemble (Phase 4)\n",
    "\n",
    "### ¿Por qué ensemble de 30 modelos?\n",
    "\n",
    "Un solo modelo es **frágil**: puede funcionar bien en ciertos periodos y mal en otros.\n",
    "El ensemble de 30 modelos:\n",
    "\n",
    "1. **Reduce varianza**: La decisión final es por votación mayoritaria\n",
    "2. **Mejora robustez**: Cada modelo ve datos ligeramente diferentes\n",
    "3. **Cuantifica incertidumbre**: La dispersión de votos indica confianza\n",
    "\n",
    "### Proceso\n",
    "\n",
    "```\n",
    "Datos Training (60%)\n",
    "     │\n",
    "     ├─ Fold 0: sub-muestra → Modelo 0\n",
    "     ├─ Fold 1: sub-muestra → Modelo 1\n",
    "     ├─ ...\n",
    "     └─ Fold 29: sub-muestra → Modelo 29\n",
    "\n",
    "Predicción final = Votación mayoritaria de 30 modelos\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación: beneficio del ensemble vs modelo individual\n",
    "np.random.seed(42)\n",
    "\n",
    "n_folds = 30\n",
    "n_decisions = 200  # decisiones de trading\n",
    "\n",
    "# Cada modelo tiene una precisión individual del ~58%\n",
    "individual_accuracy = 0.58\n",
    "\n",
    "# Simular votos de cada modelo\n",
    "correct_answer = np.random.choice([0, 1], n_decisions)\n",
    "model_predictions = np.array([\n",
    "    np.where(np.random.random(n_decisions) < individual_accuracy,\n",
    "             correct_answer,\n",
    "             1 - correct_answer)\n",
    "    for _ in range(n_folds)\n",
    "])\n",
    "\n",
    "# Ensemble por votación mayoritaria\n",
    "ensemble_votes = model_predictions.sum(axis=0)\n",
    "ensemble_pred = (ensemble_votes > n_folds / 2).astype(int)\n",
    "\n",
    "# Calcular precisiones\n",
    "individual_accs = [(model_predictions[i] == correct_answer).mean() for i in range(n_folds)]\n",
    "ensemble_acc = (ensemble_pred == correct_answer).mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precisión individual vs ensemble\n",
    "axes[0].bar(range(n_folds), individual_accs, color='lightsteelblue', alpha=0.8, label='Modelo individual')\n",
    "axes[0].axhline(ensemble_acc, color='red', linewidth=2, linestyle='--',\n",
    "                label=f'Ensemble ({ensemble_acc:.1%})')\n",
    "axes[0].axhline(np.mean(individual_accs), color='steelblue', linewidth=1.5, linestyle=':',\n",
    "                label=f'Media individual ({np.mean(individual_accs):.1%})')\n",
    "axes[0].set_xlabel('Fold', fontsize=11)\n",
    "axes[0].set_ylabel('Precisión', fontsize=11)\n",
    "axes[0].set_title('Precisión: Individual vs Ensemble (30 modelos)', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribución de votos del ensemble\n",
    "axes[1].hist(ensemble_votes, bins=range(0, n_folds+2), color='steelblue',\n",
    "             alpha=0.7, edgecolor='white', align='left')\n",
    "axes[1].axvline(n_folds/2, color='red', linestyle='--', label='Umbral de decisión (15)')\n",
    "axes[1].set_xlabel('Votos a favor', fontsize=11)\n",
    "axes[1].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[1].set_title('Distribución de Votos del Ensemble', fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Precisión media individual: {np.mean(individual_accs):.1%}')\n",
    "print(f'Precisión del ensemble:     {ensemble_acc:.1%}')\n",
    "print(f'Mejora: +{(ensemble_acc - np.mean(individual_accs))*100:.1f} puntos porcentuales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resumen del pipeline de entrenamiento\n",
    "\n",
    "```\n",
    "Phase 1: Descubrimiento     → Evaluación estadística de señales\n",
    "      ↓\n",
    "Phase 3: Optuna              → 50 trials de hiperparámetros (maximizar PF)\n",
    "      ↓\n",
    "Phase 4: 30-Fold Training    → Ensemble de 30 modelos PPO\n",
    "      ↓\n",
    "Phase 5: Validación Test     → Evaluación en datos nunca vistos (20%)\n",
    "```\n",
    "\n",
    "### Criterios de aprobación\n",
    "\n",
    "- PF > 1.2 en datos de test (mínimo para cubrir costes)\n",
    "- Número mínimo de trades (significancia estadística)\n",
    "- Degradación < 15% entre training y test\n",
    "\n",
    "### Lecciones aprendidas\n",
    "\n",
    "1. **La complejidad del modelo no correlaciona con mejores resultados**\n",
    "2. **El control de rollouts es más importante que la arquitectura**\n",
    "3. **El ensemble de 30 modelos reduce significativamente la varianza**\n",
    "4. **SAC fue descartado por no ser reproducible**\n",
    "\n",
    "> **Siguiente**: `04_backtesting_results.ipynb` — Resultados y métricas de backtesting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
