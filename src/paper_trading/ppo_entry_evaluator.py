"""
PPO Entry Evaluator for Multi-Symbol Paper Trading (Issue #428).

This module provides the PPOEntryEvaluator class that:
1. Loads trained PPO models for 4 forex symbols (EURUSD, GBPUSD, USDJPY, EURJPY)
2. Polls the AI Model DB for latest indicator data
3. Builds observations matching the training environment
4. Evaluates entry signals based on PPO predictions
5. Returns structured entry signals for downstream processing

The evaluator supports:
- Configurable confidence threshold for signal filtering
- Configurable polling interval
- Multi-timeframe observation space (M30, H1, H2, H4, H6, H8, H12, D1)
- Graceful handling of missing data or models

Usage:
    evaluator = PPOEntryEvaluator(
        symbols=["EURUSD", "GBPUSD", "USDJPY", "EURJPY"],
        model_dir="results/ppo_entry_352_production",
        confidence_threshold=0.6,
        polling_interval=60
    )

    # Poll all symbols for entry signals
    signals = evaluator.poll_all_symbols()

    # Or evaluate a single symbol
    signal = evaluator.evaluate_entry("EURUSD")
"""

import hashlib
import logging
import os
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

import numpy as np

if TYPE_CHECKING:
    from src.paper_trading.db_decision_logger import TradeDecisionDBLogger

logger = logging.getLogger(__name__)


# =============================================================================
# Constants
# =============================================================================

# Default symbols for multi-symbol trading
DEFAULT_SYMBOLS: list[str] = ["EURUSD", "GBPUSD", "USDJPY", "EURJPY"]

# Default timeframes for observation building
DEFAULT_TIMEFRAMES: list[str] = ["M30", "H1", "H2", "H4", "H6", "H8", "H12", "D1"]

# Primary timeframe for signal generation
PRIMARY_TIMEFRAME: str = "M30"

# Indicator table mapping per symbol
# EURUSD uses legacy table name, others follow pattern
INDICATOR_TABLES: dict[str, str] = {
    "EURUSD": "technical_indicators",
    "GBPUSD": "technical_indicator_gbpusd",
    "USDJPY": "technical_indicator_usdjpy",
    "EURJPY": "technical_indicator_eurjpy",
    "XAGUSD": "technical_indicator_xagusd",
    "USDCAD": "technical_indicator_usdcad",
    "EURCAD": "technical_indicator_eurcad",
    "EURGBP": "technical_indicator_eurgbp",  # Issue #571: Added for future deployment
    "USDCHF": "technical_indicator_usdchf",  # Issue #571: Added for future deployment
    "BTCUSD": "technical_indicator_btcusd",
}

# Action to direction mapping from PPO output
# 0 = HOLD (no entry), 1 = LONG, 2 = SHORT
ACTION_TO_DIRECTION: dict[int, str] = {
    0: "HOLD",
    1: "LONG",
    2: "SHORT",
}

# Default confidence threshold for signal filtering
DEFAULT_CONFIDENCE_THRESHOLD: float = 0.5

# Default polling interval in seconds
DEFAULT_POLLING_INTERVAL: int = 60

# Observation space dimensions (must match training environment)
# These constants match src/pattern_system/rl/entry_env.py
BASE_FEATURES_DIM: int = 96  # Multi-TF technical indicators
POSITION_CONTEXT_DIM: int = 5  # bars_since_signal, atr, confluence, session, momentum
VOLATILITY_FEATURES_DIM: int = 3  # ATR percentile, z-score, regime
PATTERN_FEATURES_DIM: int = (
    73  # cluster_onehot(17) + regime_onehot(4) + support(1) + history(3*17)
)

# Total observation dimension: 96 + 5 + 3 + 73 = 177
OBSERVATION_DIM: int = (
    BASE_FEATURES_DIM
    + POSITION_CONTEXT_DIM
    + VOLATILITY_FEATURES_DIM
    + PATTERN_FEATURES_DIM
)

# Pattern gating configuration (matches ObservationConfig from observation_builder.py)
MAX_CLUSTERS: int = 17
NUM_REGIMES: int = 4
HISTORY_LENGTH: int = 3

# Indicator columns needed for base features (same as entry_env.py)
INDICATOR_COLUMNS: list[str] = [
    "sma_20",
    "sma_50",
    "sma_200",
    "ema_12",
    "ema_26",
    "ema_50",
    "rsi_14",
    "atr_14",
    "bb_upper_20",
    "bb_middle_20",
    "bb_lower_20",
    "macd_line",
    "macd_signal",
    "macd_histogram",
    "stoch_k",
    "stoch_d",
]


# =============================================================================
# Data Classes
# =============================================================================


@dataclass
class EntrySignal:
    """Entry signal generated by PPO model evaluation.

    Attributes:
        symbol: Trading symbol (e.g., "EURUSD")
        direction: Trade direction ("LONG" or "SHORT")
        confidence: Confidence score (0.0 to 1.0)
        timestamp: UTC timestamp of signal generation
        candle_time: Timestamp of the candle this signal is based on (for dedup)
        observation_data: Raw observation data for debugging
        model_version: Version identifier of the PPO model used
        model_key: Key used to look up model (e.g., "Q1", "EURUSD")
        model_path: File path to the model used for this signal
        model_sha256: SHA256 hash of the model file for audit traceability
    """

    symbol: str
    direction: str
    confidence: float
    timestamp: datetime
    candle_time: datetime
    observation_data: dict[str, Any]
    model_version: str
    model_key: str = ""
    model_path: str = ""
    model_sha256: str = ""

    def __post_init__(self) -> None:
        """Validate entry signal fields."""
        if self.direction not in ["LONG", "SHORT"]:
            raise ValueError(
                f"Invalid direction: {self.direction}. Must be LONG or SHORT."
            )
        if not 0.0 <= self.confidence <= 1.0:
            raise ValueError(
                f"Confidence must be between 0.0 and 1.0, got {self.confidence}"
            )


# =============================================================================
# Main Evaluator Class
# =============================================================================


class PPOEntryEvaluator:
    """Multi-symbol PPO entry signal evaluator.

    This class loads trained PPO models and evaluates entry signals
    based on current market conditions from the indicator database.

    Attributes:
        symbols: List of trading symbols to evaluate
        model_dir: Directory containing PPO model files
        timeframes: List of timeframes for observation building
        confidence_threshold: Minimum confidence for signal generation
        polling_interval: Polling interval in seconds
        models: Dictionary mapping symbol to loaded PPO model
        model_paths: Dictionary mapping symbol to model file path
        logger: Logger instance for this evaluator
    """

    def __init__(
        self,
        symbols: Optional[list[str]] = None,
        model_dir: str = "results/ppo_entry_352_production",
        timeframes: Optional[list[str]] = None,
        confidence_threshold: float = DEFAULT_CONFIDENCE_THRESHOLD,
        polling_interval: int = DEFAULT_POLLING_INTERVAL,
        decision_logger: Optional["TradeDecisionDBLogger"] = None,
    ) -> None:
        """Initialize PPOEntryEvaluator.

        Args:
            symbols: List of symbols to evaluate. Defaults to DEFAULT_SYMBOLS.
            model_dir: Directory containing PPO model files.
            timeframes: List of timeframes for observation. Defaults to DEFAULT_TIMEFRAMES.
            confidence_threshold: Minimum confidence for signal generation (0.0-1.0).
            polling_interval: Polling interval in seconds.
            decision_logger: Optional TradeDecisionDBLogger for logging safety events.
        """
        self.symbols: list[str] = symbols if symbols is not None else DEFAULT_SYMBOLS
        self.model_dir: str = model_dir
        self.timeframes: list[str] = (
            timeframes if timeframes is not None else DEFAULT_TIMEFRAMES
        )
        self.confidence_threshold: float = confidence_threshold
        self.polling_interval: int = polling_interval

        # Model storage
        self.models: dict[str, Any] = {}
        self.model_paths: dict[str, str] = {}
        self.model_hashes: dict[str, str] = {}  # SHA256 hashes for model traceability

        # Safety: Track symbols disabled due to missing validated models
        # CRITICAL: These symbols will NOT trade until explicit model mapping exists
        self.disabled_symbols: set[str] = set()

        # Decision logger for database logging of safety events
        self.decision_logger: Optional["TradeDecisionDBLogger"] = decision_logger

        # Logger
        self.logger = logging.getLogger(f"{__name__}.PPOEntryEvaluator")

        # Database manager (lazy initialization)
        self.db_manager: Optional[Any] = None

        # Load models
        self._discover_models()
        self._load_models()

        # Log initialization
        self.logger.info(
            f"PPOEntryEvaluator initialized: symbols={self.symbols}, "
            f"model_dir={self.model_dir}, confidence_threshold={self.confidence_threshold}, "
            f"polling_interval={self.polling_interval}s"
        )

    def _discover_models(self) -> None:
        """Discover available PPO model files in model directory."""
        model_path = Path(self.model_dir)

        if not model_path.exists():
            self.logger.warning(f"Model directory does not exist: {self.model_dir}")
            return

        # Look for PPO model zip files
        # Models are quarterly: ppo_entry_q1.zip, ppo_entry_q2.zip, etc.
        for model_file in model_path.glob("ppo_entry_*.zip"):
            # Extract quarter identifier (q1, q2, q3, q4)
            quarter = model_file.stem.split("_")[-1]  # e.g., "q1"
            self.model_paths[quarter] = str(model_file)
            self.logger.info(f"Discovered model: {quarter} -> {model_file}")

        if not self.model_paths:
            self.logger.warning(f"No PPO models found in {self.model_dir}")

    def _load_models(self) -> None:
        """Load PPO models from discovered paths.

        Uses stable-baselines3 PPO.load() to load models.
        Models are shared across symbols (same architecture).
        """
        if not self.model_paths:
            return

        try:
            from stable_baselines3 import PPO

            for quarter, path in self.model_paths.items():
                try:
                    model = PPO.load(path)
                    self.models[quarter] = model
                    self.logger.info(f"Loaded PPO model: {quarter} from {path}")

                    # Compute SHA256 hash for model traceability
                    try:
                        with open(path, "rb") as f:
                            model_hash = hashlib.sha256(f.read()).hexdigest()
                        self.model_hashes[quarter] = model_hash
                        self.logger.info(
                            f"Model {quarter} SHA256: {model_hash[:16]}..."
                        )
                    except Exception as hash_err:
                        self.logger.warning(
                            f"Could not compute hash for {quarter}: {hash_err}"
                        )
                        self.model_hashes[quarter] = "unknown"

                except Exception as e:
                    self.logger.error(f"Failed to load model {quarter}: {e}")

            # SAFETY: Only map validated FX symbols to their models
            # CRITICAL: NO fallback mapping - unvalidated symbols are DISABLED
            validated_symbols = {"EURUSD", "GBPUSD", "USDJPY", "EURJPY"}

            if self.models:
                # Use first available model ONLY for validated FX symbols
                default_model_key = list(self.models.keys())[0]
                for symbol in self.symbols:
                    if symbol in validated_symbols:
                        # Validated symbol - assign model
                        if symbol not in self.models:
                            self.models[symbol] = self.models[default_model_key]
                            self.model_paths[symbol] = self.model_paths.get(
                                default_model_key, ""
                            )
                            self.model_hashes[symbol] = self.model_hashes.get(
                                default_model_key, "unknown"
                            )
                    else:
                        # SAFETY: Unvalidated symbol - DISABLE trading
                        self.disabled_symbols.add(symbol)
                        reason = (
                            f"No validated model exists for {symbol}. "
                            f"Create and validate a {symbol}-specific model before enabling."
                        )
                        self.logger.error(
                            f"SAFETY: Symbol {symbol} - TRADING DISABLED. {reason}"
                        )

                        # Log to database if decision_logger is available
                        if self.decision_logger is not None:
                            self.decision_logger.log_symbol_disabled(symbol, reason)

            # Log summary of disabled symbols
            if self.disabled_symbols:
                self.logger.warning(
                    f"DISABLED SYMBOLS (no validated models): {sorted(self.disabled_symbols)}"
                )

        except ImportError:
            self.logger.warning("stable-baselines3 not installed, models not loaded")

    def get_model_info(self) -> dict[str, Any]:
        """Get information about loaded models.

        Returns:
            Dictionary with model status information.
        """
        return {
            "symbols": self.symbols,
            "model_dir": self.model_dir,
            "loaded_models": list(self.models.keys()),
            "model_paths": self.model_paths,
            "confidence_threshold": self.confidence_threshold,
            "polling_interval": self.polling_interval,
            "timeframes": self.timeframes,
        }

    def _fetch_indicator_data(self, symbol: str) -> Optional[dict[str, dict[str, Any]]]:
        """Fetch latest indicator data from AI Model database.

        Args:
            symbol: Trading symbol to fetch data for.

        Returns:
            Dictionary mapping timeframe to indicator data, or None if unavailable.
        """
        if symbol not in INDICATOR_TABLES:
            self.logger.error(f"Unknown symbol: {symbol}")
            return None

        table_name = INDICATOR_TABLES[symbol]

        # Initialize database connection if needed
        if self.db_manager is None:
            try:
                from database.connection_manager import DatabaseManager

                self.db_manager = DatabaseManager()
                self.db_manager.connect()
            except Exception as e:
                self.logger.error(f"Failed to connect to database: {e}")
                return None

        try:
            data: dict[str, dict[str, Any]] = {}

            for timeframe in self.timeframes:
                # Query latest indicator row for this timeframe
                query = f"""
                    SELECT *
                    FROM {table_name}
                    WHERE timeframe = :timeframe
                    ORDER BY timestamp DESC
                    LIMIT 1
                """

                results = self.db_manager.execute_query(
                    "ai_model", query, {"timeframe": timeframe}
                )

                if results:
                    data[timeframe] = dict(results[0])
                else:
                    self.logger.debug(f"No data for {symbol}/{timeframe}")

            return data if data else None

        except Exception as e:
            self.logger.error(f"Failed to fetch indicator data for {symbol}: {e}")
            return None

    def get_observation(self, symbol: str) -> Optional[np.ndarray]:
        """Build 177-dim observation array matching training environment.

        The observation structure must match entry_env.py (Issue #341):
        - Base features: 96 dims (multi-TF technical indicators)
        - Position context: 5 dims (bars, atr, confluence, session, momentum)
        - Volatility features: 3 dims (ATR percentile, z-score, regime)
        - Pattern gating: 73 dims (cluster 17 + regime 4 + support 1 + history 51)
        Total: 177 dims

        Args:
            symbol: Trading symbol to build observation for.

        Returns:
            Numpy array with 177-dim observation, or None if data unavailable.
        """
        # Fetch indicator data
        data = self._fetch_indicator_data(symbol)

        if data is None:
            self.logger.warning(f"No indicator data available for {symbol}")
            return None

        # 1. Build base features (96 dims)
        base_features = self._build_base_features(data)

        # 2. Build position context (5 dims)
        position_context = self._build_position_context(data)

        # 3. Build volatility features (3 dims)
        volatility_features = self._build_volatility_features(data)

        # 4. Build pattern gating features (73 dims)
        pattern_features = self._build_pattern_features(data, symbol)

        # Concatenate all feature blocks
        observation = np.concatenate(
            [
                base_features,  # 96 dims
                position_context,  # 5 dims
                volatility_features,  # 3 dims
                pattern_features,  # 73 dims
            ]
        )  # Total: 177 dims

        # Clean up NaN/Inf values
        observation = np.nan_to_num(observation, nan=0.0, posinf=0.0, neginf=0.0)

        return observation.astype(np.float32)

    def _build_observation_from_data(
        self, data: dict[str, dict[str, Any]]
    ) -> Optional[np.ndarray]:
        """Build 177-dim observation from pre-fetched indicator data.

        This is a helper method used by evaluate_entry() to build observations
        from already-fetched data, avoiding duplicate database calls.

        Args:
            data: Dictionary mapping timeframe to indicator data.

        Returns:
            Numpy array with 177-dim observation, or None if data invalid.
        """
        try:
            # 1. Build base features (96 dims)
            base_features = self._build_base_features(data)

            # 2. Build position context (5 dims)
            position_context = self._build_position_context(data)

            # 3. Build volatility features (3 dims)
            volatility_features = self._build_volatility_features(data)

            # 4. Build pattern gating features (73 dims)
            # Note: Pattern features need symbol, but we can use generic pattern
            # when called from evaluate_entry context
            pattern_features = self._build_pattern_features_generic(data)

            # Concatenate all feature blocks
            observation = np.concatenate(
                [
                    base_features,  # 96 dims
                    position_context,  # 5 dims
                    volatility_features,  # 3 dims
                    pattern_features,  # 73 dims
                ]
            )  # Total: 177 dims

            # Clean up NaN/Inf values
            observation = np.nan_to_num(observation, nan=0.0, posinf=0.0, neginf=0.0)

            return observation.astype(np.float32)

        except Exception as e:
            self.logger.error(f"Failed to build observation from data: {e}")
            return None

    def _build_pattern_features_generic(
        self, data: dict[str, dict[str, Any]]
    ) -> np.ndarray:
        """Build 73-dim pattern features without symbol-specific patterns.

        This is used when building observations from pre-fetched data.

        Args:
            data: Dictionary mapping timeframe to indicator data.

        Returns:
            73-dim float32 array of pattern features.
        """
        features = np.zeros(PATTERN_FEATURES_DIM, dtype=np.float32)

        # Get primary timeframe data for pattern detection
        tf_data = data.get(PRIMARY_TIMEFRAME, {})

        # Cluster features (17 dims) - use indicator-based proxies
        rsi = float(tf_data.get("rsi_14", 50.0) or 50.0)
        macd = float(tf_data.get("macd", 0.0) or 0.0)
        macd_signal = float(tf_data.get("macd_signal", 0.0) or 0.0)

        # Simple cluster assignment based on RSI
        cluster_idx = min(int((rsi / 100.0) * 17), 16)
        features[cluster_idx] = 1.0  # One-hot cluster

        # Regime features (4 dims) - indices 17-20
        # Based on RSI ranges: oversold, neutral-low, neutral-high, overbought
        if rsi < 30:
            features[17] = 1.0  # Oversold
        elif rsi < 50:
            features[18] = 1.0  # Neutral-low
        elif rsi < 70:
            features[19] = 1.0  # Neutral-high
        else:
            features[20] = 1.0  # Overbought

        # Support/resistance feature (1 dim) - index 21
        close = float(tf_data.get("close", 0.0) or 0.0)
        sma_20 = float(tf_data.get("sma_20", close) or close)
        if close > 0 and sma_20 > 0:
            features[21] = (close - sma_20) / sma_20  # Distance from SMA

        # History features (51 dims) - indices 22-72
        # Use rolling statistics from available data
        atr = float(tf_data.get("atr_14", 0.0) or 0.0)
        bb_upper = float(tf_data.get("bb_upper", 0.0) or 0.0)
        bb_lower = float(tf_data.get("bb_lower", 0.0) or 0.0)

        # Fill history features with available metrics
        if atr > 0:
            features[22] = atr  # ATR
        if bb_upper > 0 and bb_lower > 0 and close > 0:
            bb_width = (bb_upper - bb_lower) / close
            features[23] = bb_width  # BB width
            features[24] = (close - bb_lower) / (
                bb_upper - bb_lower + 1e-8
            )  # BB position

        # MACD histogram
        features[25] = macd - macd_signal

        return features

    def _build_base_features(self, data: dict[str, dict[str, Any]]) -> np.ndarray:
        """Build 96-dim base features from multi-timeframe indicators.

        Features are extracted from primary timeframe (M30) and context timeframes
        matching the training environment structure.

        Args:
            data: Dictionary mapping timeframe to indicator data.

        Returns:
            96-dim float32 array of base features.
        """
        features = np.zeros(BASE_FEATURES_DIM, dtype=np.float32)

        # Primary timeframe (M30) features - first 32 dims
        primary_tf = PRIMARY_TIMEFRAME
        if primary_tf in data:
            tf_data = data[primary_tf]
            idx = 0

            # 16 indicator features
            for col in INDICATOR_COLUMNS:
                if idx < 16:
                    # Map column names to data keys
                    key = col.replace("_20", "").replace("_14", "")
                    if col == "atr_14":
                        key = "atr_14"
                    elif col == "rsi_14":
                        key = "rsi_14"
                    elif col in tf_data:
                        key = col

                    value = tf_data.get(key, tf_data.get(col, 0.0))
                    features[idx] = float(value) if value is not None else 0.0
                    idx += 1

            # Remaining primary features (price-derived)
            close = float(tf_data.get("close", 0.0))
            open_price = float(tf_data.get("open", 0.0))
            high = float(tf_data.get("high", 0.0))
            low = float(tf_data.get("low", 0.0))

            if close > 0:
                features[16] = (close - open_price) / close  # Price change
                features[17] = (high - low) / close  # Range
                features[18] = (close - low) / (high - low + 1e-8)  # Position in range

        # Context timeframes (H1, H4) features - 32-64 and 64-96
        context_tfs = ["H1", "H4"]
        offset = 32

        for ctx_tf in context_tfs:
            if ctx_tf in data:
                tf_data = data[ctx_tf]
                for i, col in enumerate(INDICATOR_COLUMNS[:16]):
                    if offset + i < BASE_FEATURES_DIM:
                        key = col.replace("_20", "").replace("_14", "")
                        value = tf_data.get(key, tf_data.get(col, 0.0))
                        features[offset + i] = (
                            float(value) if value is not None else 0.0
                        )
            offset += 32

        return features

    def _build_position_context(self, data: dict[str, dict[str, Any]]) -> np.ndarray:
        """Build 5-dim position context features.

        Features:
        - bars_since_signal: Normalized (0-1), default 0 for new signals
        - atr: Current ATR value
        - confluence: Indicator alignment strength (0-1)
        - session_indicator: Sine wave based on time of day
        - momentum: Price change rate in pips

        Args:
            data: Dictionary mapping timeframe to indicator data.

        Returns:
            5-dim float32 array of position context features.
        """
        import time

        # Get primary timeframe data
        tf_data = data.get(PRIMARY_TIMEFRAME, {})

        # bars_since_signal: 0 for new entry evaluation
        bars_since_signal = 0.0

        # ATR
        atr = float(tf_data.get("atr_14", tf_data.get("atr", 0.0)) or 0.0)

        # Confluence from indicator alignment
        confluence = self._calculate_confluence(tf_data)

        # Session indicator (sine wave)
        current_time = time.time()
        session_indicator = np.sin(current_time / 86400.0 * 2 * np.pi)

        # Momentum (price change in pips)
        close = float(tf_data.get("close", 0.0) or 0.0)
        open_price = float(tf_data.get("open", 0.0) or 0.0)
        if close > 0 and open_price > 0:
            momentum = (close - open_price) * 10000  # Convert to pips
        else:
            momentum = 0.0

        return np.array(
            [
                bars_since_signal,
                atr,
                confluence,
                session_indicator,
                momentum,
            ],
            dtype=np.float32,
        )

    def _build_volatility_features(self, data: dict[str, dict[str, Any]]) -> np.ndarray:
        """Build 3-dim volatility features.

        Features (matching Issue #336):
        - ATR percentile: Current ATR rank in rolling window (0-1)
        - ATR z-score: Deviation from rolling mean (-5 to 5)
        - Volatility regime: 0.0=low, 0.5=medium, 1.0=high

        Args:
            data: Dictionary mapping timeframe to indicator data.

        Returns:
            3-dim float32 array of volatility features.
        """
        tf_data = data.get(PRIMARY_TIMEFRAME, {})
        atr = float(tf_data.get("atr_14", tf_data.get("atr", 0.0)) or 0.0)

        # For live inference, use simple heuristics
        # In production, these would come from rolling statistics
        atr_percentile = 0.5  # Default to medium
        atr_zscore = 0.0  # Default to mean
        volatility_regime = 0.5  # Default to medium

        # Simple regime classification based on ATR magnitude
        if atr > 0:
            # Assume forex typical ATR ranges (adjust per symbol)
            if atr < 0.0008:  # Low volatility
                atr_percentile = 0.25
                volatility_regime = 0.0
            elif atr > 0.0020:  # High volatility
                atr_percentile = 0.75
                volatility_regime = 1.0

        return np.array(
            [
                atr_percentile,
                atr_zscore,
                volatility_regime,
            ],
            dtype=np.float32,
        )

    def _build_pattern_features(
        self, data: dict[str, dict[str, Any]], symbol: str
    ) -> np.ndarray:
        """Build 73-dim pattern gating features.

        Features (matching ObservationConfig from observation_builder.py):
        - Cluster one-hot: 17 dims
        - Regime one-hot: 4 dims
        - Support level: 1 dim (normalized)
        - Cluster history: 51 dims (3 * 17 one-hot)

        Args:
            data: Dictionary mapping timeframe to indicator data.
            symbol: Trading symbol for pattern lookup.

        Returns:
            73-dim float32 array of pattern features.
        """
        # Initialize feature blocks
        cluster_onehot = np.zeros(MAX_CLUSTERS, dtype=np.float32)
        regime_onehot = np.zeros(NUM_REGIMES, dtype=np.float32)
        support_level = np.array([0.0], dtype=np.float32)
        cluster_history = np.zeros(HISTORY_LENGTH * MAX_CLUSTERS, dtype=np.float32)

        # Default cluster: use neutral middle cluster
        default_cluster = MAX_CLUSTERS // 2  # Cluster 8
        cluster_onehot[default_cluster] = 1.0

        # Default regime: range (index 2)
        # Regime indices: 0=trend_favorable, 1=trend_adverse, 2=range, 3=spike
        regime_onehot[2] = 1.0  # Default to range

        # Default support: 0.5 (medium)
        support_level[0] = 0.5

        # Default history: repeat default cluster for each history slot
        for i in range(HISTORY_LENGTH):
            offset = i * MAX_CLUSTERS
            cluster_history[offset + default_cluster] = 1.0

        # Concatenate all pattern features
        pattern_features = np.concatenate(
            [
                cluster_onehot,  # 17 dims
                regime_onehot,  # 4 dims
                support_level,  # 1 dim
                cluster_history,  # 51 dims
            ]
        )  # Total: 73 dims

        return pattern_features

    def _calculate_confluence(self, tf_data: dict[str, Any]) -> float:
        """Calculate indicator confluence strength.

        Returns a score [0, 1] based on indicator alignment.

        Args:
            tf_data: Indicator data for a single timeframe.

        Returns:
            Confluence score between 0.0 and 1.0.
        """
        signals = 0
        total = 0

        # RSI signal
        rsi = float(tf_data.get("rsi_14", tf_data.get("rsi", 50.0)) or 50.0)
        if rsi < 30:  # Oversold
            signals += 1
            total += 1
        elif rsi > 70:  # Overbought
            signals -= 1
            total += 1

        # MACD signal
        macd = float(tf_data.get("macd", tf_data.get("macd_line", 0.0)) or 0.0)
        macd_signal = float(tf_data.get("macd_signal", 0.0) or 0.0)
        if abs(macd - macd_signal) > 0.00001:
            if macd > macd_signal:
                signals += 1
            else:
                signals -= 1
            total += 1

        # Stochastic signal
        stoch_k = float(tf_data.get("stoch_k", 50.0) or 50.0)
        if stoch_k < 20:  # Oversold
            signals += 1
            total += 1
        elif stoch_k > 80:  # Overbought
            signals -= 1
            total += 1

        if total > 0:
            return abs(signals / total)
        return 0.5

    def _predict(self, symbol: str, observation: np.ndarray) -> tuple[int, float]:
        """Get prediction from PPO model.

        Args:
            symbol: Trading symbol for model selection.
            observation: Observation array.

        Returns:
            Tuple of (action, confidence).
        """
        if symbol not in self.models:
            self.logger.warning(f"No model loaded for {symbol}")
            return 0, 0.0  # HOLD with 0 confidence

        model = self.models[symbol]

        try:
            # Get action from model
            action, _states = model.predict(observation, deterministic=True)

            # Get action probabilities for confidence
            # This requires accessing the policy's distribution
            try:
                obs_tensor = model.policy.obs_to_tensor(observation.reshape(1, -1))[0]
                distribution = model.policy.get_distribution(obs_tensor)
                probs = distribution.distribution.probs.detach().cpu().numpy()[0]
                action_idx = (
                    int(action.item()) if hasattr(action, "item") else int(action)
                )
                confidence = float(probs[action_idx])
            except Exception:
                # Fallback if we can't get probabilities
                confidence = 0.5

            # Handle numpy array action
            action_value = (
                int(action.item()) if hasattr(action, "item") else int(action)
            )
            return action_value, confidence

        except Exception as e:
            self.logger.error(f"Prediction failed for {symbol}: {e}")
            return 0, 0.0

    def evaluate_entry(self, symbol: str) -> Optional[EntrySignal]:
        """Evaluate entry signal for a symbol.

        Args:
            symbol: Trading symbol to evaluate.

        Returns:
            EntrySignal if signal generated, None otherwise.
        """
        # SAFETY: Hard block disabled symbols - NO trading without validated model
        if symbol in self.disabled_symbols:
            self.logger.error(
                f"SAFETY BLOCK: {symbol} is DISABLED - no validated model exists. "
                f"Create and validate a {symbol}-specific model before trading."
            )
            return None

        # Check if model is loaded
        if symbol not in self.models:
            self.logger.debug(f"No model for {symbol}, skipping evaluation")
            return None

        # Fetch indicator data directly to get candle timestamp for dedup
        data = self._fetch_indicator_data(symbol)

        if data is None:
            self.logger.debug(f"No indicator data available for {symbol}")
            return None

        # Extract candle_time from primary timeframe for dedup tracking
        candle_time: Optional[datetime] = None
        if PRIMARY_TIMEFRAME in data:
            ts = data[PRIMARY_TIMEFRAME].get("timestamp")
            if ts is not None:
                if isinstance(ts, datetime):
                    candle_time = ts
                else:
                    # Handle string timestamp from database
                    try:
                        candle_time = datetime.fromisoformat(
                            str(ts).replace("Z", "+00:00")
                        )
                    except (ValueError, TypeError):
                        self.logger.warning(f"Failed to parse candle timestamp: {ts}")

        if candle_time is None:
            self.logger.warning(f"No candle timestamp available for {symbol}")
            return None

        # Build observation from data
        observation = self._build_observation_from_data(data)

        if observation is None:
            self.logger.debug(f"Failed to build observation for {symbol}")
            return None

        # Get prediction
        action, confidence = self._predict(symbol, observation)

        # Check if action is entry (not HOLD)
        direction = ACTION_TO_DIRECTION.get(action, "HOLD")

        if direction == "HOLD":
            self.logger.debug(f"{symbol}: HOLD signal (confidence={confidence:.2f})")
            return None

        # Check confidence threshold
        if confidence < self.confidence_threshold:
            self.logger.debug(
                f"{symbol}: {direction} below threshold "
                f"(confidence={confidence:.2f} < {self.confidence_threshold})"
            )
            return None

        # Get model traceability info for this symbol
        model_key = (
            symbol
            if symbol in self.model_paths
            else self._get_model_key_for_symbol(symbol)
        )
        model_path = self.model_paths.get(model_key, self.model_paths.get(symbol, ""))
        model_sha256 = self.model_hashes.get(
            model_key, self.model_hashes.get(symbol, "unknown")
        )

        # Create entry signal with candle_time for dedup and model traceability
        signal = EntrySignal(
            symbol=symbol,
            direction=direction,
            confidence=confidence,
            timestamp=datetime.now(timezone.utc),
            candle_time=candle_time,
            observation_data={
                "observation_shape": observation.shape,
                "observation_mean": float(observation.mean()),
                "candle_time": candle_time.isoformat(),
            },
            model_version=self._get_model_version(symbol),
            model_key=model_key,
            model_path=str(model_path),
            model_sha256=model_sha256,
        )

        self.logger.info(
            f"Entry signal: {symbol} {direction} (confidence={confidence:.2f}, "
            f"candle_time={candle_time.isoformat()})"
        )

        return signal

    def _get_model_key_for_symbol(self, symbol: str) -> str:
        """Get the model key used for a specific symbol.

        For validated FX symbols, this returns the quarterly model key
        that was assigned during model loading.

        Args:
            symbol: Trading symbol (e.g., "EURUSD").

        Returns:
            Model key string (e.g., "Q1", "EURUSD", or "unknown").
        """
        # If symbol is directly in models, use it as the key
        if symbol in self.models:
            # Check if symbol maps to a quarterly model
            symbol_model = self.models[symbol]
            for quarter in ["Q1", "Q2", "Q3", "Q4", "q1", "q2", "q3", "q4"]:
                if quarter in self.models and self.models[quarter] is symbol_model:
                    return quarter
            return symbol
        return "unknown"

    def _get_model_version(self, symbol: str) -> str:
        """Get model version identifier for a symbol.

        Args:
            symbol: Trading symbol.

        Returns:
            Model version string.
        """
        # Find which quarter model is used
        for quarter, model in self.models.items():
            if quarter in ["q1", "q2", "q3", "q4"] and self.models.get(symbol) is model:
                return quarter
        return "unknown"

    def poll_all_symbols(self) -> list[EntrySignal]:
        """Poll all symbols for entry signals.

        Returns:
            List of generated entry signals.
        """
        signals: list[EntrySignal] = []

        for symbol in self.symbols:
            try:
                signal = self.evaluate_entry(symbol)
                if signal is not None:
                    signals.append(signal)
            except Exception as e:
                self.logger.error(f"Error evaluating {symbol}: {e}")

        if signals:
            self.logger.info(f"Generated {len(signals)} entry signals")

        return signals
